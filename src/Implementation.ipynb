{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4fb0c80-f375-4ad8-86ea-d0e5f54bbad3",
   "metadata": {},
   "source": [
    "## Preprocessing and Data Pipeline\n",
    "\n",
    "The preprocessing pipeline prepares question tokens and image features as described in the paper.\n",
    "\n",
    "- **Question Processing**:\n",
    "  - Questions are tokenized to a maximum length of 14 tokens[^1].\n",
    "  - Words are embedded using 300-dimensional GloVe vectors[^1].\n",
    "  - A single-layer LSTM encodes the sequence into 512-dimensional question features[^1].\n",
    "\n",
    "- **Image Processing**:\n",
    "  - Images are passed through a ResNeXt-152 CNN pretrained on Visual Genome to extract 8×8 grid features (64 regions), each of 2048 dimensions[^1].\n",
    "  - These features are projected to 512-dimensional vectors to match the question features[^1].\n",
    "\n",
    "- **Answer Representation**:\n",
    "  - The candidate answer set is fixed to the top 3129 most frequent answers[^1].\n",
    "  - Ground-truth answers are encoded as multi-hot vectors with soft scores, enabling multi-label classification[^1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951246fc-edf8-4675-a74a-0f43886ea1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Example utility: load GloVe embeddings into a dictionary\n",
    "def load_glove_embeddings(glove_path):\n",
    "    glove_dict = {}\n",
    "    with open(glove_path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            if len(parts) == 0: \n",
    "                continue\n",
    "            word = parts[0]\n",
    "            vec = np.array(parts[1:], dtype=np.float32)\n",
    "            glove_dict[word] = vec\n",
    "    return glove_dict\n",
    "\n",
    "# Build vocabulary for questions (word2idx) and answers (answer2idx) from training data\n",
    "def build_vocab_and_answers(questions, answers, glove_dict, min_freq=1, top_answers=3129):\n",
    "    # Build question word vocabulary (include only words present in GloVe for consistency)\n",
    "    word_counts = {}\n",
    "    for q in questions:\n",
    "        for w in q.lower().split():\n",
    "            if w in glove_dict:  # only consider words with glove vectors\n",
    "                word_counts[w] = word_counts.get(w, 0) + 1\n",
    "    # Include words with frequency >= min_freq\n",
    "    vocab = ['<pad>', '<unk>']  # special tokens\n",
    "    vocab += [w for w, cnt in word_counts.items() if cnt >= min_freq]\n",
    "    word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "    # Build answer vocabulary (top_answers by frequency)\n",
    "    ans_counts = {}\n",
    "    for ans_list in answers:\n",
    "        for ans in ans_list:\n",
    "            ans_counts[ans] = ans_counts.get(ans, 0) + 1\n",
    "    # sort answers by frequency and take top N\n",
    "    top_ans = sorted(ans_counts.items(), key=lambda x: x[1], reverse=True)[:top_answers]\n",
    "    answer_vocab = [ans for ans, cnt in top_ans]\n",
    "    answer2idx = {ans: i for i, ans in enumerate(answer_vocab)}\n",
    "    return word2idx, answer2idx\n",
    "\n",
    "class VQAv2Dataset(Dataset):\n",
    "    def __init__(self, questions_json, annotations_json, image_dir, glove_dict, word2idx, answer2idx):\n",
    "        # Load question and annotation data\n",
    "        with open(questions_json, 'r') as f_q:\n",
    "            questions_data = json.load(f_q)\n",
    "        with open(annotations_json, 'r') as f_a:\n",
    "            ann_data = json.load(f_a)\n",
    "        # Map question id to question and image, and to answers\n",
    "        self.entries = []\n",
    "        ann_map = {ann['question_id']: ann for ann in ann_data['annotations']}\n",
    "        for q in questions_data['questions']:\n",
    "            q_id = q['question_id']\n",
    "            img_id = q['image_id']\n",
    "            question_str = q['question']\n",
    "            # Collect answers (10 answers per question in VQA v2)\n",
    "            if q_id in ann_map:\n",
    "                ans_objs = ann_map[q_id]['answers']\n",
    "                answers = [ans_obj['answer'] for ans_obj in ans_objs]\n",
    "            else:\n",
    "                answers = []  # for test questions (no answers)\n",
    "            self.entries.append({\n",
    "                'question_id': q_id,\n",
    "                'image_id': img_id,\n",
    "                'question': question_str,\n",
    "                'answers': answers\n",
    "            })\n",
    "        self.image_dir = image_dir\n",
    "        self.word2idx = word2idx\n",
    "        self.answer2idx = answer2idx\n",
    "        self.glove_dict = glove_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.entries[idx]\n",
    "        # Tokenize question\n",
    "        tokens = entry['question'].lower().split()  # simple split; could use more advanced tokenization\n",
    "        # Convert tokens to indices with padding/truncation to length 14\n",
    "        max_q_len = 14\n",
    "        tok_idxs = []\n",
    "        for t in tokens[:max_q_len]:\n",
    "            tok_idxs.append(self.word2idx.get(t, self.word2idx.get('<unk>')))\n",
    "        pad_length = max_q_len - len(tok_idxs)\n",
    "        if pad_length > 0:\n",
    "            tok_idxs += [self.word2idx['<pad>']] * pad_length\n",
    "        else:\n",
    "            tok_idxs = tok_idxs[:max_q_len]\n",
    "        question_indices = torch.tensor(tok_idxs, dtype=torch.long)\n",
    "        # Prepare answer multi-hot vector\n",
    "        ans_vector = torch.zeros(len(self.answer2idx), dtype=torch.float)\n",
    "        if entry['answers']:\n",
    "            # Use soft score: each answer gets credit = min(count/3, 1)&#8203;:contentReference[oaicite:8]{index=8}&#8203;:contentReference[oaicite:9]{index=9}\n",
    "            ans_count = {}\n",
    "            for ans in entry['answers']:\n",
    "                if ans in self.answer2idx:\n",
    "                    ans_count[ans] = ans_count.get(ans, 0) + 1\n",
    "            for ans, count in ans_count.items():\n",
    "                score = min(count / 3, 1.0)\n",
    "                ans_idx = self.answer2idx.get(ans)\n",
    "                if ans_idx is not None:\n",
    "                    ans_vector[ans_idx] = score\n",
    "        # Load image and preprocess (assuming images are COCO, adjust path accordingly)\n",
    "        image_path = f\"{self.image_dir}/COCO_train2014_{entry['image_id']:012d}.jpg\"  # example for train2014\n",
    "        # Image loading and transformation will be handled in a collate or in model (for efficiency, do in model forward)\n",
    "        # We return image path or ID for later loading\n",
    "        return image_path, question_indices, ans_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e150c05b-d0a8-4c00-84a4-656def0831c0",
   "metadata": {},
   "source": [
    "## Dataset Handling: `VQAv2Dataset`\n",
    "\n",
    "In the code above, the `VQAv2Dataset` class performs the following preprocessing steps:\n",
    "\n",
    "- **Question Handling**:\n",
    "  - Loads questions and annotations[^1].\n",
    "  - Tokenizes each question.\n",
    "  - Pads or truncates tokens to a fixed length of 14[^1].\n",
    "\n",
    "- **Answer Vector Construction**:\n",
    "  - Constructs a multi-label answer vector.\n",
    "  - Applies soft target scoring between 0 and 1 for each answer, using the standard VQA strategy[^1].\n",
    "\n",
    "- **Image Handling**:\n",
    "  - Image loading is deferred for efficiency.\n",
    "  - The dataset returns only an image path or ID.\n",
    "  - Actual image feature extraction is handled later in the model via a CNN[^1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74759d97-5793-43a4-a7a6-c1637250f536",
   "metadata": {},
   "source": [
    "# Model Architecture Components\n",
    "\n",
    "We now define the building blocks of the LRCN model. The model uses a Transformer-like co-attention architecture with a **Layer-Residual Mechanism (LRM)** to preserve information across layers.\n",
    "\n",
    "There are two types of attention sub-layers:\n",
    "\n",
    "### 1. Self-Attention (SA) Block\n",
    "Operates on a single modality (text or image) to capture **intra-modal features**.\n",
    "\n",
    "### 2. Guided-Attention (GA) Block\n",
    "A cross-attention that uses one modality to guide attention in the other, capturing **inter-modal features** between image and question.\n",
    "\n",
    "### Layer-Residual Mechanism (LRM)\n",
    "\n",
    "This mechanism adds a direct residual connection from the output of an attention block in the previous layer to the output of the corresponding block in the current layer. In other words, each SA or GA block receives an extra skip input from its counterpart in the previous layer, mitigating information loss through deep layers.\n",
    "\n",
    "Formally, if \\( X_{l-1} \\) is the output from the previous layer and `PrevRe` is the output from the previous layer's same type of block, then the LRM is applied as:\n",
    "\n",
    "$$\n",
    "X_l = \\text{LayerNorm}(X_{l-1} + \\text{PrevRe} + \\text{MHA}(Q_{l-1}, K_{l-1}, V_{l-1}))\n",
    "$$\n",
    "\n",
    "where `MHA` denotes multi-head attention.\n",
    "\n",
    "We implement this by adding the previous layer's output of the same block type (`prev_output`) in addition to the standard residual connection. Below we define PyTorch modules for **Multi-Head Attention with LRM**. We use `nn.MultiheadAttention` for the attention operation and apply residual connections and layer normalization as described (Post-LN architecture).\n",
    "\n",
    "We provide separate classes for self-attention and guided-attention blocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327dd6e-2929-4cba-9f64-241da683f61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttentionBlock(nn.Module):\n",
    "    \"\"\"Self-Attention block with Layer-Residual Mechanism (LRM) for one modality.\"\"\"\n",
    "    def __init__(self, hidden_dim=512, num_heads=8):\n",
    "        super(SelfAttentionBlock, self).__init__()\n",
    "        # Multi-head self-attention (queries=keys=values=input sequence)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.ln = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x, prev_sa_output=None):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch, seq_len, hidden_dim) - input features for this layer.\n",
    "        prev_sa_output: Tensor of same shape as x, output from previous layer's SA block (for LRM).\n",
    "        \"\"\"\n",
    "        # Multi-head self-attention (with residual connection)\n",
    "        attn_out, _ = self.mha(x, x, x)  # Self-attend: Q=K=V=x\n",
    "        out = x + attn_out  # primary residual: add input features&#8203;:contentReference[oaicite:20]{index=20}\n",
    "        if prev_sa_output is not None:\n",
    "            out = out + prev_sa_output  # add skip connection from previous layer's SA&#8203;:contentReference[oaicite:21]{index=21}\n",
    "        out = self.ln(out)  # layer normalization after residual sum (Post-LN)&#8203;:contentReference[oaicite:22]{index=22}\n",
    "        return out\n",
    "\n",
    "class GuidedAttentionBlock(nn.Module):\n",
    "    \"\"\"Guided (Cross) Attention block with LRM, guiding features of one modality with another.\"\"\"\n",
    "    def __init__(self, hidden_dim=512, num_heads=8):\n",
    "        super(GuidedAttentionBlock, self).__init__()\n",
    "        # Multi-head attention for cross-modal: will use queries from one modality and keys/values from another\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.ln = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, query_seq, context_seq, prev_ga_output=None):\n",
    "        \"\"\"\n",
    "        query_seq: Tensor (batch, L_q, hidden_dim) - features to be updated (queries).\n",
    "        context_seq: Tensor (batch, L_c, hidden_dim) - guiding features (keys and values).\n",
    "        prev_ga_output: Tensor of same shape as query_seq, output from previous layer's GA block (for LRM).\n",
    "        \"\"\"\n",
    "        # Multi-head cross-attention: query attends to context\n",
    "        attn_out, _ = self.mha(query_seq, context_seq, context_seq)  # Q=query_seq, K=context_seq, V=context_seq\n",
    "        out = query_seq + attn_out  # add primary residual connection (input to output)&#8203;:contentReference[oaicite:23]{index=23}\n",
    "        if prev_ga_output is not None:\n",
    "            out = out + prev_ga_output  # add skip from previous layer's GA output&#8203;:contentReference[oaicite:24]{index=24}\n",
    "        out = self.ln(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b7bbf0-03ab-461f-ad51-f163f40e07c1",
   "metadata": {},
   "source": [
    "In the `SelfAttentionBlock`, we pass `prev_sa_output` (the previous layer's self-attention output for the same modality) to incorporate the **Layer-Residual Mechanism (LRM)**. \n",
    "\n",
    "In the `GuidedAttentionBlock`, `prev_ga_output` carries the previous layer's guided-attention output (for the same query modality).\n",
    "\n",
    "Both blocks add their immediate input (`x` or `query_seq`) as well as the skip connection (`prev_*_output`) to the attention output before normalization, matching equations (8)–(10) in the paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff979de-3d06-4b77-9e95-95ffbfd3d804",
   "metadata": {},
   "source": [
    "# LRCN Model Variants: Pure-Stacking, Co-Stacking, Encoder–Decoder\n",
    "\n",
    "The LRCN model can be configured in three stacking variants as discussed in the paper:\n",
    "\n",
    "### Encoder–Decoder (E-D)\n",
    "The question features are fully encoded with self-attention layers, then the final question representation guides the image features via a guided-attention layer (similar to a traditional Transformer encoder-decoder).  \n",
    "This means the image is only attended once by the final question features.\n",
    "\n",
    "### Pure-Stacking\n",
    "The question and image features interact at every layer. At each layer:\n",
    "- The question features are first refined by a self-attention block.\n",
    "- Then, the image features are updated by a guided-attention block using the same layer's question output.\n",
    "\n",
    "The question is not directly updated by the image in this variant; image guidance happens progressively with increasingly refined question features.\n",
    "\n",
    "### Co-Stacking\n",
    "A bidirectional co-attention is applied at every layer. At each layer:\n",
    "- After the question self-attention, **textual co-attention** is applied (question features are guided by image features) to inject visual information into the question representation.\n",
    "- Then, **visual guided-attention** is applied (image features are guided by the updated question).\n",
    "\n",
    "This allows early and reciprocal interactions: image guiding text and text guiding image at each layer, enabling richer feature fusion.\n",
    "\n",
    "---\n",
    "\n",
    "We implement a single `LRCNModel` class that can switch between these variants. The model consists of:\n",
    "\n",
    "### Word Embedding + LSTM Encoder\n",
    "- Embeds the input question tokens using preloaded **GloVe** weights.\n",
    "- Encodes them into an initial question feature sequence \\( Y \\) of length \\( m = 14 \\) and dimension 512.\n",
    "\n",
    "### CNN Feature Extractor\n",
    "- Processes the input image to produce a grid of \\( n = 64 \\) visual features of dimension 512.\n",
    "- We use **ResNeXt-152** (pretrained), truncated to output a \\( 16 \\times 16 \\) feature map.\n",
    "- This is then downsampled to \\( 8 \\times 8 \\) via a 2×2 stride-2 convolution, as described in the paper.\n",
    "- A linear layer reduces 2048-dimensional features to 512-dimensional.\n",
    "\n",
    "### Stacked Co-Attention Layers\n",
    "- A stack of \\( L \\) layers (we allow \\( L = 6 \\) or \\( 8 \\), as per the paper) of **Self-Attention (SA)** and **Guided-Attention (GA)** blocks with **Layer-Residual Mechanism (LRM)**.\n",
    "- The exact sequence per layer depends on the variant:\n",
    "\n",
    "| Variant         | Layer Sequence                                                                 |\n",
    "|-----------------|----------------------------------------------------------------------------------|\n",
    "| Encoder–Decoder | \\( L \\) SA layers on question only, then 1 GA layer on image.                   |\n",
    "| Pure-Stacking   | For each layer: Question SA → Image GA.                                         |\n",
    "| Co-Stacking     | For each layer: Question SA → Text GA (Q guided by image) → Image GA (X guided by question). |\n",
    "\n",
    "### Multimodal Fusion & Classifier\n",
    "- After \\( L \\) layers, we obtain final refined question features \\( Y^{(L)} \\) and image features \\( X^{(L)} \\).\n",
    "- These are **pooled via attention pooling** (described in the next section) to get a single vector each for image and question.\n",
    "- The vectors are fused and passed through a classifier to predict the **answer scores**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f9682d-e8a6-4868-9f47-d956b136a907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class LRCNModel(nn.Module):\n",
    "    def __init__(self, hidden_dim=512, num_heads=8, num_layers=6, \n",
    "                 vocab_size=10000, glove_weights=None, answer_vocab_size=3129, \n",
    "                 variant=\"pure\"):\n",
    "        \"\"\"\n",
    "        variant: \"enc_dec\", \"pure\", or \"co\" indicating Encoder-Decoder, Pure-Stacking, or Co-Stacking structure.\n",
    "        num_layers: number of layers L for SA (and GA in stacking). For Encoder-Decoder, this is number of question SA layers.\n",
    "        \"\"\"\n",
    "        super(LRCNModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.variant = variant\n",
    "\n",
    "        # Question embedding: 300-d GloVe embeddings, project to hidden_dim via LSTM\n",
    "        self.embedding = nn.Embedding(vocab_size, 300, padding_idx=0)\n",
    "        if glove_weights is not None:\n",
    "            # Initialize embedding weights with pre-trained GloVe (freeze or fine-tune as needed)\n",
    "            self.embedding.weight.data.copy_(torch.tensor(glove_weights, dtype=torch.float32))\n",
    "            self.embedding.weight.requires_grad = False  # Freeze GloVe embeddings (optional)\n",
    "        self.question_lstm = nn.LSTM(input_size=300, hidden_size=hidden_dim, batch_first=True, bidirectional=False)\n",
    "\n",
    "        # Image feature extractor: ResNeXt-152 backbone pre-trained (we will cut after conv5 layer)\n",
    "        backbone = models.resnext152_32x8d(pretrained=True)\n",
    "        # Remove classifier and avgpool to get convolutional feature map\n",
    "        self.cnn_backbone = nn.Sequential(*list(backbone.children())[:-2])  # shape: (batch, 2048, H=14?, W=14?)\n",
    "        # Additional conv to go from 14x14 to 8x8 if needed (pad to 16x16 then conv stride2)&#8203;:contentReference[oaicite:44]{index=44}\n",
    "        self.downsample_conv = nn.Conv2d(in_channels=2048, out_channels=2048, kernel_size=2, stride=2)\n",
    "        # Linear projection to 512-d\n",
    "        self.img_feat_proj = nn.Linear(2048, hidden_dim)\n",
    "\n",
    "        # Define attention blocks for L layers\n",
    "        # For Encoder-Decoder: question SA layers + one image GA (so num_layers SA, 1 GA)\n",
    "        # For Pure-Stacking: L layers of question SA and image GA\n",
    "        # For Co-Stacking: L layers of question SA, text GA, image GA\n",
    "        self.sa_blocks = nn.ModuleList([SelfAttentionBlock(hidden_dim, num_heads) for _ in range(num_layers)])\n",
    "        if variant == \"enc_dec\":\n",
    "            # Only one GA block for the final cross-attention in Encoder-Decoder\n",
    "            self.ga_image_block = GuidedAttentionBlock(hidden_dim, num_heads)\n",
    "        elif variant == \"pure\":\n",
    "            # Pure-stacking: L GA blocks for image (question guides image at each layer)\n",
    "            self.ga_blocks = nn.ModuleList([GuidedAttentionBlock(hidden_dim, num_heads) for _ in range(num_layers)])\n",
    "        elif variant == \"co\":\n",
    "            # Co-stacking: L GA blocks for text (image guides question) and L GA blocks for image (question guides image)\n",
    "            self.ga_text_blocks = nn.ModuleList([GuidedAttentionBlock(hidden_dim, num_heads) for _ in range(num_layers)])\n",
    "            self.ga_image_blocks = nn.ModuleList([GuidedAttentionBlock(hidden_dim, num_heads) for _ in range(num_layers)])\n",
    "        else:\n",
    "            raise ValueError(\"Unknown variant type\")\n",
    "\n",
    "        # Attention pooling layers for final feature summarization (two-layer MLP as described)&#8203;:contentReference[oaicite:45]{index=45}&#8203;:contentReference[oaicite:46]{index=46}\n",
    "        self.att_mlp_text = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        self.att_mlp_image = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        # Fusion and classifier\n",
    "        self.fusion_norm = nn.LayerNorm(hidden_dim)  # LayerNorm for fused features&#8203;:contentReference[oaicite:47]{index=47}\n",
    "        self.fusion_Wx = nn.Linear(hidden_dim, hidden_dim)  # W_x for image feature&#8203;:contentReference[oaicite:48]{index=48}\n",
    "        self.fusion_Wy = nn.Linear(hidden_dim, hidden_dim)  # W_y for question feature&#8203;:contentReference[oaicite:49]{index=49}\n",
    "        self.classifier = nn.Linear(hidden_dim, answer_vocab_size)  # projects fused feature to answer logits&#8203;:contentReference[oaicite:50]{index=50}\n",
    "\n",
    "        # Image preprocessing transform (to normalize images as ResNeXt expects)\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize(448),  # ensure image is large enough (ResNeXt was trained on 224x224, but VG training might use larger)\n",
    "            transforms.CenterCrop(448),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def forward(self, image_paths, question_indices):\n",
    "        batch_size = question_indices.size(0)\n",
    "        # 1. Question embedding encoding\n",
    "        embed = self.embedding(question_indices)  # (B, 14, 300)\n",
    "        # Pack sequence for LSTM (though all seq len =14 here, we can skip packing for simplicity)\n",
    "        lstm_out, _ = self.question_lstm(embed)   # (B, 14, 512)\n",
    "        # Initial question features Y(0)\n",
    "        q_feats = lstm_out  # we'll treat this as Y^(0) (already contextualized by LSTM)&#8203;:contentReference[oaicite:51]{index=51}\n",
    "        # 2. Image feature extraction\n",
    "        # Load and preprocess images\n",
    "        imgs = [self.image_transform(Image.open(path).convert('RGB')) for path in image_paths]\n",
    "        imgs_tensor = torch.stack(imgs, dim=0)  # (B, 3, H, W)\n",
    "        # CNN backbone to get conv features\n",
    "        conv_feats = self.cnn_backbone(imgs_tensor)  # (B, 2048, Hc, Wc), ideally Hc=Wc≈14 or 15\n",
    "        # If feature map is not 16x16, pad to 16 and downsample to 8x8 as described\n",
    "        # (In practice, a 448x448 input to ResNeXt-152 yields ~14x14 feature map. We pad to 16x16 then conv stride2 to get 8x8.)\n",
    "        if conv_feats.size(2) < 16:\n",
    "            # pad to 16x16\n",
    "            pad_h = 16 - conv_feats.size(2)\n",
    "            pad_w = 16 - conv_feats.size(3)\n",
    "            conv_feats = F.pad(conv_feats, (0, pad_w, 0, pad_h))\n",
    "        grid_feats = self.downsample_conv(conv_feats)  # (B, 2048, 8, 8)\n",
    "        grid_feats = grid_feats.view(batch_size, 2048, -1).permute(0, 2, 1)  # (B, 64, 2048) flatten spatial\n",
    "        # Project to 512-dim\n",
    "        grid_feats = self.img_feat_proj(grid_feats)  # (B, 64, 512)\n",
    "        # Initial image features X(0)\n",
    "        v_feats = grid_feats\n",
    "\n",
    "        # 3. Layer-residual co-attention stacks\n",
    "        # Initialize skip connection storages for LRM\n",
    "        prev_sa_out = None    # previous layer's question SA output\n",
    "        prev_ga_img_out = None  # previous layer's image GA output\n",
    "        prev_ga_txt_out = None  # (co-stacking) previous layer's text GA output\n",
    "        # Iterate through L layers\n",
    "        for l in range(self.num_layers):\n",
    "            # Self-attention on question features (Y) for this layer\n",
    "            q_out = self.sa_blocks[l](q_feats, prev_sa_output=prev_sa_out)  # SA on question&#8203;:contentReference[oaicite:52]{index=52}\n",
    "            # Update prev_sa_out for next iteration\n",
    "            prev_sa_out = q_out\n",
    "            if self.variant == \"enc_dec\":\n",
    "                # Encoder-Decoder: Only perform question SA in all layers; guided attention will be done once after loop\n",
    "                q_feats = q_out\n",
    "                continue  # skip GA within loop\n",
    "            elif self.variant == \"pure\":\n",
    "                # Pure-Stacking: guided attention on image, using current question output\n",
    "                v_out = self.ga_blocks[l](v_feats, q_out, prev_ga_output=prev_ga_img_out)  # GA: image features guided by question&#8203;:contentReference[oaicite:53]{index=53}\n",
    "                # Update prev outputs and current features\n",
    "                prev_ga_img_out = v_out\n",
    "                q_feats = q_out    # question features carry to next layer (not updated by image in pure stacking)\n",
    "                v_feats = v_out\n",
    "            elif self.variant == \"co\":\n",
    "                # Co-Stacking: two-stage GA at each layer\n",
    "                # Stage 1: Text GA - update question using image as context\n",
    "                q_co_out = self.ga_text_blocks[l](q_out, v_feats, prev_ga_output=prev_ga_txt_out)  # question guided by image\n",
    "                # Stage 2: Image GA - update image using updated question as context\n",
    "                v_out = self.ga_image_blocks[l](v_feats, q_co_out, prev_ga_output=prev_ga_img_out)  # image guided by question\n",
    "                # Update prev outputs for next layer\n",
    "                prev_ga_txt_out = q_co_out\n",
    "                prev_ga_img_out = v_out\n",
    "                # Set up for next iteration\n",
    "                q_feats = q_co_out  # updated question features go to next layer\n",
    "                v_feats = v_out\n",
    "        # End of layers loop\n",
    "\n",
    "        if self.variant == \"enc_dec\":\n",
    "            # After encoding question for L layers, do one guided attention on image with final question output\n",
    "            # (Image features v_feats still initial X(0) here)\n",
    "            v_feats = self.ga_image_block(v_feats, q_feats, prev_ga_output=None)\n",
    "            # In E-D, q_feats is final question from encoder, v_feats is final image after one cross-attention\n",
    "        # At this point, we have final question features in q_feats (B, m, 512) and image features in v_feats (B, n, 512)\n",
    "\n",
    "        # 4. Attention pooling: compute attention weights over image and question features&#8203;:contentReference[oaicite:54]{index=54}\n",
    "        # and produce attended feature vectors X_bar and Y_bar as weighted sums&#8203;:contentReference[oaicite:55]{index=55}.\n",
    "        # Compute attention logits\n",
    "        img_att_logits = self.att_mlp_image(v_feats)  # (B, n, 1)\n",
    "        txt_att_logits = self.att_mlp_text(q_feats)   # (B, m, 1)\n",
    "        # Attention weights\n",
    "        img_att_weights = F.softmax(img_att_logits, dim=1)  # (B, n, 1), softmax over image regions&#8203;:contentReference[oaicite:56]{index=56}\n",
    "        txt_att_weights = F.softmax(txt_att_logits, dim=1)  # (B, m, 1), softmax over question words&#8203;:contentReference[oaicite:57]{index=57}\n",
    "        # Weighted sum to get single feature vectors\n",
    "        v_att = torch.sum(img_att_weights * v_feats, dim=1)  # (B, 512), visual attended feature \\bar{X}&#8203;:contentReference[oaicite:58]{index=58}\n",
    "        q_att = torch.sum(txt_att_weights * q_feats, dim=1)  # (B, 512), textual attended feature \\bar{Y}&#8203;:contentReference[oaicite:59]{index=59}\n",
    "\n",
    "        # 5. Feature fusion and answer prediction\n",
    "        # Linear fusion with layer normalization&#8203;:contentReference[oaicite:60]{index=60}\n",
    "        fused = self.fusion_norm(self.fusion_Wx(v_att) + self.fusion_Wy(q_att))  # fused feature z&#8203;:contentReference[oaicite:61]{index=61}\n",
    "        # Predict answer scores\n",
    "        logits = self.classifier(fused)  # raw scores f for each answer&#8203;:contentReference[oaicite:62]{index=62}\n",
    "        # Apply activation: ReLU + Sigmoid for multi-label classification as per paper&#8203;:contentReference[oaicite:63]{index=63}\n",
    "        logits = F.relu(logits)         # ReLU on logits&#8203;:contentReference[oaicite:64]{index=64}\n",
    "        probs = torch.sigmoid(logits)   # Sigmoid to get probabilities&#8203;:contentReference[oaicite:65]{index=65}\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca59aa55-db47-49af-84d6-b973b1cbbeb3",
   "metadata": {},
   "source": [
    "## A Few Notes on the Implementation\n",
    "\n",
    "- We use `prev_sa_out`, `prev_ga_img_out`, and `prev_ga_txt_out` to carry the **LRM skip connections** between layers for:\n",
    "  - Self-Attention (SA)\n",
    "  - Image Guided-Attention (GA)\n",
    "  - Text Guided-Attention (GA), respectively.\n",
    "\n",
    "- In **Pure-Stacking**, only `prev_sa_out` and `prev_ga_img_out` are used (since text GA is not present).\n",
    "\n",
    "- In **Encoder–Decoder**, only `prev_sa_out` is used to form the question SA chain.\n",
    "\n",
    "- In **Co-Stacking**, each layer:\n",
    "  1. First updates the **question** with **image context** using `ga_text_blocks`.\n",
    "  2. Then updates the **image** with the new question using `ga_image_blocks`.\n",
    "\n",
    "  This matches the description of applying **textual co-attention after self-attention** to incorporate visual guidance early.\n",
    "\n",
    "---\n",
    "\n",
    "### CNN Feature Extraction\n",
    "\n",
    "- Uses a **ResNeXt-152** backbone.\n",
    "- The feature map is padded to \\( 16 \\times 16 \\) and a stride-2 convolution is applied to obtain an \\( 8 \\times 8 \\) grid of features.\n",
    "- Each of the 64 grid cells is a **2048-dimensional** vector, which is projected to **512 dimensions** so that \\( X \\) (image features) and \\( Y \\) (question features) share the same dimension.\n",
    "\n",
    "---\n",
    "\n",
    "### Attention Pooling\n",
    "\n",
    "- Uses a learned **MLP** to produce attention weights for each feature vector.\n",
    "- A **softmax** is applied to get attention weights \\( \\alpha_i \\) and \\( \\beta_j \\) as in equations (15)–(16) of the paper.\n",
    "\n",
    "---\n",
    "\n",
    "### Final Fusion & Output\n",
    "\n",
    "- The final fused vector is computed as:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fbe79d-cb5c-4019-9fe1-fa8f443ba687",
   "metadata": {},
   "source": [
    "## Training Configuration and Loop\n",
    "\n",
    "We train the model using **Binary Cross-Entropy (BCE) loss** with **sigmoid outputs** for multi-label answer prediction.\n",
    "\n",
    "- **Optimizer**: Adam  \n",
    "  - \\( \\beta_1 = 0.9 \\), \\( \\beta_2 = 0.98 \\), as specified in the paper.\n",
    "\n",
    "- **Learning Rate Schedule**:\n",
    "  - **Warm-up** for the first 3 epochs.\n",
    "  - **Decay** at epochs 10 and 12.\n",
    "\n",
    "- **Training Duration**:\n",
    "  - A total of **13 epochs** on the combined **train + val** datasets (optionally including **Visual Genome**), as described in the paper.\n",
    "\n",
    "- **Batch Size**: 64\n",
    "\n",
    "---\n",
    "\n",
    "Below is a high-level description of the training loop incorporating these settings:\n",
    "\n",
    "1. Initialize model, optimizer (Adam), and BCE loss.\n",
    "2. Apply warm-up learning rate schedule for the first 3 epochs.\n",
    "3. Train the model for 13 epochs:\n",
    "   - At each epoch:\n",
    "     - Perform forward and backward passes.\n",
    "     - Update weights using Adam.\n",
    "     - Log training loss and accuracy.\n",
    "     - Run validation after each epoch.\n",
    "4. Decay learning rate at epochs 10 and 12.\n",
    "5. Track and log validation performance for model selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97186f0-b6da-4eb9-a2a4-9cf50d13b35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training components\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LRCNModel(hidden_dim=512, num_heads=8, num_layers=6, \n",
    "                  vocab_size=len(word2idx), glove_weights=glove_matrix, \n",
    "                  answer_vocab_size=len(answer2idx), variant=\"co\").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4, betas=(0.9, 0.98))\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy for multi-label classification\n",
    "\n",
    "# Learning rate warm-up and decay settings\n",
    "def adjust_learning_rate(epoch):\n",
    "    # Warm-up for first 3 epochs: linearly scale lr from 1/4 to 1 (since initial set at 1e-4 as max)\n",
    "    if epoch < 3:\n",
    "        lr_scale = (epoch + 1) / 3.0  # epoch starts at 0\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 1e-4 * lr_scale\n",
    "    # Decay by 1/5 at epoch 10 and 12\n",
    "    if epoch == 10 or epoch == 12:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 0.2  # decay learning rate&#8203;:contentReference[oaicite:82]{index=82}\n",
    "\n",
    "# Example DataLoader (assuming dataset and collate are defined)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(13):  # train for 13 epochs&#8203;:contentReference[oaicite:83]{index=83}\n",
    "    model.train()\n",
    "    adjust_learning_rate(epoch)\n",
    "    total_loss = 0.0\n",
    "    for i, (image_paths, questions, targets) in enumerate(train_loader):\n",
    "        questions = questions.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # Forward pass\n",
    "        probs = model(image_paths, questions)  # forward (image_paths used inside model)\n",
    "        loss = criterion(probs, targets)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        # (Optional: logging every N batches)\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0; total_questions = 0\n",
    "    with torch.no_grad():\n",
    "        for image_paths, questions, targets in val_loader:\n",
    "            questions = questions.to(device)\n",
    "            targets = targets.to(device)\n",
    "            probs = model(image_paths, questions)\n",
    "            loss = criterion(probs, targets)\n",
    "            val_loss += loss.item()\n",
    "            # Compute accuracy (e.g., whether any ground truth answer is predicted as top-1)\n",
    "            # For multi-label, we can consider top answer correctness for simplicity\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            # If any of the ground truth answers (those with target > 0) matches the prediction, count as correct\n",
    "            for j in range(preds.size(0)):\n",
    "                true_answers = (targets[j] > 0).nonzero(as_tuple=True)[0]\n",
    "                if preds[j].item() in true_answers.cpu().tolist():\n",
    "                    correct += 1\n",
    "            total_questions += questions.size(0)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    accuracy = correct / total_questions * 100.0\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {avg_loss:.4f}, Val Loss = {avg_val_loss:.4f}, Val Accuracy = {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90333020-4ff5-469d-bd52-cb58cd21d58c",
   "metadata": {},
   "source": [
    "## Notes on the Training Loop\n",
    "\n",
    "- We **adjust the learning rate** at the start of each epoch:\n",
    "  - Linearly **increase** it during the first 3 epochs (warm-up).\n",
    "  - **Decay** it by a factor of 5 at epochs 10 and 12.\n",
    "\n",
    "- We use **`BCELoss`** on the **sigmoid outputs (`probs`)** for multi-label classification, since each question can have multiple correct answers.\n",
    "\n",
    "- **Validation Accuracy**:\n",
    "  - We compute a simple accuracy metric by checking if the top predicted answer is among the ground truth answers.\n",
    "  - (More rigorous VQA-specific metrics can be applied, but this serves well for monitoring.)\n",
    "\n",
    "- **Logging**:\n",
    "  - After each epoch, we log the **average training loss**, **validation loss**, and **accuracy**.\n",
    "\n",
    "---\n",
    "\n",
    "With this setup, the model is ready for training and experimentation. \n",
    "\n",
    "- The **default configuration** uses:\n",
    "  - `6` layers\n",
    "  - the **Co-Stacking** variant\n",
    "\n",
    "- You can easily modify:\n",
    "  - `variant=\"pure\"` or `variant=\"enc_dec\"`\n",
    "  - `num_layers=8` (for deeper stacks)\n",
    "\n",
    "to explore the different LRCN variants described in the paper.\n",
    "\n",
    "### Architectural and Hyperparameter Defaults:\n",
    "- 8 attention heads  \n",
    "- 512-dimensional features  \n",
    "- Layer-Residual connections  \n",
    "- Training schedule from Section 4.2 of the LRCN paper\n",
    "\n",
    "---\n",
    "\n",
    "### Sources\n",
    "\n",
    "- D. Han et al., *\"LRCN: Layer-residual Co-Attention Networks for visual question answering,\"*  \n",
    "  *Expert Systems With Applications, vol. 263, 2025*  \n",
    "  (Includes architecture and preprocessing details)\n",
    "\n",
    "- Equation references **(8)–(18)** from the LRCN paper for model computations.\n",
    "\n",
    "- Training hyperparameter settings taken from **Section 4.2** of the LRCN paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a2c3b4-6be4-4c49-9465-da7f321c2c36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
